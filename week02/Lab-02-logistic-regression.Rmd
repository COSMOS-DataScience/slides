---
title: "Logistic Regression in R"
author: "Mine Dogecu"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css", "https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"]
    lib_dir: libs
    seal: false
    nature:
      beforeInit: "cols_macro.js"
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"

---

class: title-slide

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(fabricerin)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(gtsummary)
```

<br>
<br>
.right-panel[ 

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`

]



<style type="text/css">

body, td {
   font-size: 14px;
}
code.r{
  font-size: 20px;
}
pre {
  font-size: 20px
}
</style>


```{r echo = FALSE, message = FALSE}
library(tidyverse)

arthritis <- read_csv("https://raw.githubusercontent.com/cosmos-uci-dshs/data/main/RheumArth_Tx_AgeComparisons.csv") %>% 
  janitor::clean_names() %>% 
  mutate(sex = case_when(sex == 0 ~ "female",
                   sex == 1 ~ "male")) %>% 
  mutate(sex = as.factor(sex)) %>% 
  mutate(age_gp = case_when(age_gp == 1 ~ "control",
                   age_gp == 2 ~ "elderly")) %>% 
  mutate(age_gp = as.factor(age_gp)) %>% 
  mutate(cdai_yn = case_when(cdai_yn == 1 ~ "no",
                             cdai_yn == 2 ~ "yes")) %>%
  mutate(cdai_yn = as.factor(cdai_yn))
```

```{r echo = FALSE, message = FALSE}
alzheimer_data <- read_csv("https://raw.githubusercontent.com/COSMOS-DataScience/slides/main/data/alzheimer_data.csv")
```


---
* Remember from the lecture that we are fitting a regression model with a binary outcome. 

* As such, the model is as follows:

$$\begin{eqnarray*}
\log \Big(\frac{\hat{p}}{1- \hat{p}} \Big) & = & a + b_{1}x_1 + \ldots + b_{q}x_{q}
\end{eqnarray*}$$


* The left hand side of this model is the logarithm of the odds of success.

* Thereby, the probability of success of $\pi$ can be written as follows:

$$\begin{eqnarray*}
\hat{p} & = & \frac{\exp(a + b_{1}x_1 + \ldots + b_{q}x_{q})}{1 + \exp(a + b_{1}x_1 + \ldots + b_{q}x_{q})}
\end{eqnarray*}$$

* The above means once we estimate the coefficients of the model, we can estimate the probability of success of the outcome of interest.

---
* Let's revisit Alzheimer's data set, and consider the task of building a logistic regression model with diagnosis as its response variable and variables age, education, naccicv, and female as its predictors.

* Let's begin by transforming the response to a new feature with two categories: no symptoms (0) versus mild or strong
symptoms (1):


```{r warning=FALSE}
alzheimer_data <- alzheimer_data %>% 
  mutate(diag = ifelse(diagnosis %in% c(1, 2), "1", "0"),
         diag = as.factor(diag))
```

* Running a logistic regression model in R is pretty straightforward. Before we do that, we should notice female is a binary variable as well. As such, we should make sure R recognizes that feature as a factor variable.

```{r warning=FALSE}
alzheimer_data <- alzheimer_data %>%
  mutate(female=as.factor(female))

```
---
```{r warning=FALSE}
logistic_model <-glm(diag ~ educ + age + naccicv + female, family=binomial, data=alzheimer_data)
summary(logistic_model)
```

---

```{r warning=FALSE}
glm(diag ~ educ + age + naccicv + female, family=binomial, data=alzheimer_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3), exponentiate = TRUE)

```



---


### Logistic Regression Model Evaluation:


To split the data into training and validation sets using the rsample package in R, you can use the initial_split() function. Here's an example of how you can split the data:

```{r message=FALSE, warning=FALSE}
library(rsample)

set.seed(0)
data_split <- initial_split(alzheimer_data, prop = 0.7) 

train_data <- training(data_split)
test_data <- testing(data_split)

```


#### As we saw, next step after splitting the data into train and test would be training the model using training data:

```{r warning=FALSE}
logistic_model2 <- glm(diag ~ educ + age + naccicv + female, family=binomial, data=train_data)
summary(logistic_model2)
```
---

```{r warning=FALSE}
glm(diag ~ educ + age + naccicv + female, family=binomial, data=train_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3), exponentiate = TRUE)

```
---

* Followed, by testing it via the validation set. This means to calculate the probability of success for each subject in the test set:


```{r warning=FALSE}
pred_prob <- logistic_model2 %>% 
  predict(test_data,type="response")
```


* We are now ready to calculate the accuracy of our trained model. To accomplish this, we translate all probabilities of success above to 0.5 to a 1 and otherwise to a 0, followed by tracking the number of correct predictions (1's correctly predicted as 1's and 0's correctly predicted as 0's).

```{r}
predicted.classes <- ifelse(pred_prob > 0.5, "1", "0")
mean(predicted.classes == test_data$diag)
```

* This model yields a 65% accuracy rate!