---
title: "Linear Regression in R"
author: "Dr. Mine Dogucu"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css", "https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"]
    lib_dir: libs
    seal: false
    nature:
      beforeInit: "cols_macro.js"
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"
editor_options: 
  chunk_output_type: console
---

class: title-slide

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(fabricerin)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(gtsummary)
```

<br>
<br>
.right-panel[ 

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`

]



<style type="text/css">

body, td {
   font-size: 14px;
}
code.r{
  font-size: 20px;
}
pre {
  font-size: 20px
}
</style>

---

class:inverse middle

.font180[Linear Regression]

---

```{r echo = FALSE, message = FALSE}
library(tidyverse)

arthritis <- read_csv("https://raw.githubusercontent.com/cosmos-uci-dshs/data/main/RheumArth_Tx_AgeComparisons.csv") %>% 
  janitor::clean_names() %>% 
  mutate(sex = case_when(sex == 0 ~ "female",
                   sex == 1 ~ "male")) %>% 
  mutate(sex = as.factor(sex)) %>% 
  mutate(age_gp = case_when(age_gp == 1 ~ "control",
                   age_gp == 2 ~ "elderly")) %>% 
  mutate(age_gp = as.factor(age_gp)) %>% 
  mutate(cdai_yn = case_when(cdai_yn == 1 ~ "no",
                             cdai_yn == 2 ~ "yes")) %>%
  mutate(cdai_yn = as.factor(cdai_yn))
```

```{r echo = FALSE, message = FALSE}
alzheimer_data <- read_csv("https://raw.githubusercontent.com/COSMOS-DataScience/slides/main/data/alzheimer_data.csv")
```

class: middle

* Now that you have learned about regression models, we will build a multiple regression model for predicting the left hippocampus volume of the brain, labeled as **lhippo**, through two predictors, namely **age** and **educ**.

* Remember that in the general, a multiple linear regression model with $p$ explanatory variables can
be presented as follows:

$$\begin{equation*}
\hat{y}  =  a + b_{1}x_{1} + b_{2}x_{2} + \cdots + b_{p}x_{p}.
\end{equation*}$$


* The left hand side of this model is the response variable, a numerical continuous variable.

---

class: middle


Recall the left hippocampus volume **lhippo** is likely to shrink as Alzheimer's severs. Also, from Yueqi's introduction, while the progress of the disease is a function of age, it is possible that education can have a reverse effect on the progress of the disease.

To fit linear models all we need to do is to apply the **lm()** command in R. We begin with plotting the response versus each predictor, separately.

---
```{r eval=FALSE, warning=FALSE,fig.width=16}
ggplot(data = alzheimer_data) +
  geom_point(aes(x = age, y = lhippo), color = "red") +
  labs(x = "Age", y = "left hippo") +
  theme_minimal()
```

```{r eval=FALSE, warning=FALSE,fig.width=16}
ggplot(data = alzheimer_data) +
  geom_point(aes(x = educ, y = lhippo), color = "red") +
  labs(x = "Education", y = "left hippo") +
  theme_minimal()
```

---
```{r echo=FALSE, warning=FALSE, fig.align='center',fig.width=14}
library(ggplot2)
library(gridExtra)

p1 <- ggplot(data = alzheimer_data) +
  geom_point(aes(x = age, y = lhippo), color = "red", width = 30) +
  labs(x = "Age", y = "Left Hippocampus Volume") +
  theme_minimal() +
  theme(plot.margin = margin(l = 0.5, r = 0.5))  # Adjust the left and right margins

p2 <- ggplot(data = alzheimer_data) +
  geom_point(aes(x = educ, y = lhippo), color = "red") +
  labs(x = "Education", y = "Left Hippocampus Volume") +
  theme_minimal() +
  theme(plot.margin = margin(l = 0.5, r = 0.5))  # Adjust the left and right margins

grid.arrange(p1, p2, ncol = 2)

```
---
Here is the regression of lhippo versus age:

```{r warning=FALSE, out.height=0.1}
lm_model <- lm(lhippo ~ age, data = alzheimer_data)

summary(lm_model)
```
---
```{r warning=FALSE}
lm(lhippo ~ age, data = alzheimer_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3))

```
---

Let's see the fitted line:

```{r warning=FALSE, fig.align='center', message=FALSE, fig.height=5.5}

ggplot(data = alzheimer_data, aes(x = age, y = lhippo)) +
  geom_point(color = "red") +
  geom_smooth(method = "lm", color = "blue", se=FALSE) +
  labs(x = "Age", y = "Left Hippocampus Volume")
```

---
Here is the regression of lhippo versus education:

```{r warning=FALSE}
lm_model <- lm(lhippo ~ educ, data = alzheimer_data)
summary(lm_model)
```


---
```{r}
lm(lhippo ~ educ, data = alzheimer_data) %>%
  tbl_regression()
```

---
```{r warning=FALSE}

lm(lhippo ~ educ, data = alzheimer_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3))
 
lm(lhippo ~ educ, data = alzheimer_data) %>%
  tbl_regression(estimate_fun = function(x) round(x, digits = 3))
```
---

Let's see the fitted line:


```{r warning=FALSE, message=FALSE, fig.align='center', fig.height=5}
ggplot(data = alzheimer_data, aes(x = educ, y = lhippo)) +
  geom_point(color = "red") +
  geom_smooth(method = "lm", color = "blue", se=FALSE) +
  labs(x = "Education", y = "Left Hippocampus Volume")
```
---
Here is the regression of lhippo versus age and education:

```{r warning=FALSE, out.height=0.1, fig.height=0.1, fig.width=6, fig.align='center'}
lm_model <- lm(lhippo ~ age + educ, data = alzheimer_data)
summary(lm_model)
```
---

```{r warning=FALSE}
lm(lhippo ~ age + educ, data = alzheimer_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3))

```





---

class:inverse middle

.font80[Cross-Validation]

---
class: middle

* Let's try to evaluate the performance of our model by calculating its accuracy or mean squared error (MSE), depending on whether we are dealing with a linear regression or logistic regression model.

* Cross-validation is a commonly used technique to achieve this evaluation. It is an old approach, devised by statisticians Fred Mosteller and John Tukey in 1968.

* The process involves splitting the data into training and validation (or test) sets. We then fit or train the model using the training portion of the dataset. The accuracy or MSE is then calculated by comparing the predictions made by the model on the validation set to the actual values.

---

class: middle


* For linear regression models, we typically use the mean squared error (MSE) to measure the quality of predictions. The lower the MSE, the better the model's performance. The MSE represents the average squared difference between the predicted values and the true values.

* For classification problems (e.g., logistic regression), we use accuracy as a metric to assess the model's performance. Accuracy represents the proportion of correctly classified instances out of the total instances in the validation set. The higher the accuracy, the better the model's performance.
---


To split the data into training and validation sets using the rsample package in R, you can use the initial_split() function. Here's an example of how you can split the data:

```{r message=FALSE, warning=FALSE}
library(rsample)

set.seed(0)
data_split <- initial_split(alzheimer_data, prop = 0.7) 

train_data <- training(data_split)
test_data <- testing(data_split)

```

---
### Linear Regression Model Evaluation:

#### After splitting the data into train and test, we train the model using training data:

```{r warning=FALSE}
lm_model <- lm(lhippo ~ age + educ, data = train_data)
summary(lm_model)
```
---

```{r warning=FALSE}
lm(lhippo ~ age + educ, data = train_data) %>%
  tbl_regression(estimate_fun = function(x) style_number(x, digits = 3))

```
---
Now, let's use the trained model to make predictions on the validation data:


```{r warning=FALSE}
predictions <- predict(lm_model, newdata = test_data)
```
---
To evaluate the performance of our model, we can calculate the Mean Squared Error (MSE) between the predicted values and the actual values:


```{r warning=FALSE}
mean((test_data$lhippo - predictions)^2)

```

Question:

What happen to MSE when we change the ratio we split the data?